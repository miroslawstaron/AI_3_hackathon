#!/usr/bin/env python3
#
# This file checks if the files generated by the models execute
# Purpose: Test multiple AI model endpoints by sending code generation requests
# Author: Miroslaw Staron
# Date: November 10, 2025

# Import required libraries
import argparse
import re
import pandas as pd
from tqdm import tqdm       # Import tqdm for progress bar   
import requests             # For making HTTP requests to model APIs
import json                 # For handling JSON data
import os

# Define the conversation messages for the AI model
# System message sets the model's behavior, user message contains the prompt
messages = [
            {"role": "system",
             "content": "You are a software that writes C programs based on prompts. Provides only the code, no description."},
            {"role": "user", 
             "content": "Generate a Fibonacci code"}
            ]

# Test 1: Query the first model endpoint (deepthought)
url = 'http://deepthought.cse.chalmers.se:80/api/chat'

# Prepare request payload with model parameters
data = {
    "model": "llama3.2:latest",
    "messages": messages,
    "stream": False,            # Don't stream the response
    "temperature": 0.0,         # Deterministic output (no randomness)
    }
headers = {'Content-Type': 'application/json'}

# Send POST request to the model API
response = requests.post(url, data=json.dumps(data), headers=headers)

# Check if request was successful and print first 100 characters of response
if response.status_code == 200:
    response_dict = json.loads(response.text)
    response_raw = response_dict['message']['content']
    print(f"Response from model llama3.2:latest from {url}: {response_raw[:100]}")
else:
    print(f"Request failed {url} with status code {response.status_code}: {response.text}")

# Test 2: Query the second model endpoint (deeperthought)
url = 'http://deeperthought.cse.chalmers.se:80/api/chat'
response = requests.post(url, data=json.dumps(data), headers=headers)

# Check if request was successful and print first 100 characters of response
if response.status_code == 200:
    response_dict = json.loads(response.text)
    response_raw = response_dict['message']['content']
    print(f"Response from model llama3.2:latest from {url}: {response_raw[:100]}")
else:
    print(f"Request failed {url} with status code {response.status_code}: {response.text}")

# Test 3: Query the third model endpoint (deepestthought) with a different model
url = 'http://deepestthought.cse.chalmers.se:11434/api/chat'
data = {
    "model": "gemma3:latest",   # Using Gemma model instead of Llama
    "messages": messages,
    "stream": False, 
    "temperature": 0.0,
    }
response = requests.post(url, data=json.dumps(data), headers=headers)

# Check if request was successful and print first 100 characters of response
if response.status_code == 200:
    response_dict = json.loads(response.text)
    response_raw = response_dict['message']['content']
    print(f"Response from model gemma3:latest:latest from {url}: {response_raw[:100]}")
else:
    print(f"Request failed from {url} with status code {response.status_code}: {response.text}")

# Test 4: Query the fourth model endpoint (lazythought)
url = 'http://lazythought.cse.chalmers.se:80/api/chat'
response = requests.post(url, data=json.dumps(data), headers=headers)

# Check if request was successful and print first 100 characters of response
if response.status_code == 200:
    response_dict = json.loads(response.text)
    response_raw = response_dict['message']['content']
    print(f"Response from model llama3.2:latest from {url}: {response_raw[:100]}")
else:
    print(f"Request failed {url} with status code {response.status_code}: {response.text}")